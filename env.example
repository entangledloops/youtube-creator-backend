# LLM Settings
LOCAL_LLM_URL=http://localhost:1234/v1
OPENAI_API_KEY=your_openai_api_key_here

# Toggle between "local" or "openai"
LLM_PROVIDER=local

# Model Configuration
OPENAI_MODEL=gpt-4o-mini
LLM_CONTEXT_SIZE=128000

# YouTube API (optional, for enhanced functionality)
YOUTUBE_API_KEY=your_youtube_api_key_here

# Video Cache Settings
# Set to "true" to disable video caching completely (useful for testing)
DISABLE_VIDEO_CACHE=false

# Set to "true" to use cache only for transcripts (LLM processing still runs normally)
# This allows reusing transcript data while forcing fresh LLM analysis
CACHE_TRANSCRIPTS_ONLY=false

# Output Directory Settings
# Directory where downloaded files (CSV, JSON) are saved
# Default: output
OUTPUT_DIR=output